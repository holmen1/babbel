{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wmf07eJE8p9r"
   },
   "source": [
    "# Chatbot\n",
    "\n",
    "[Reformer](https://arxiv.org/abs/2001.04451), also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the modules we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "aV4zpTnSVFIp",
    "outputId": "e3a85dd1-e375-4636-ea62-b9b403f0952a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                               1.3.4                \r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import trax   \n",
    "from trax import layers as tl\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary filename\n",
    "VOCAB_FILE = 'en_32k.subword'\n",
    "\n",
    "# vocabulary file directory\n",
    "VOCAB_DIR = 'data/vocabs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RidbAcoR6duP"
   },
   "outputs": [],
   "source": [
    "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        vocab_size (int): size of the vocabulary\n",
    "        n_layers (int): number of decoder layers\n",
    "        mode (string): setting of the model which can be 'train', 'eval', or 'predict' \n",
    "        attention_type(class): attention class to use \n",
    "    Returns: \n",
    "        model (ReformerLM): a reformer language model implemented in Trax\n",
    "    \"\"\"    \n",
    "    \n",
    "    # initialize an instance of Trax's ReformerLM class\n",
    "    model = trax.models.reformer.ReformerLM( \n",
    "        vocab_size=vocab_size,\n",
    "        n_layers=n_layers,\n",
    "        mode=mode,\n",
    "        attention_type=attention_type\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decode from a pretrained model\n",
    "\n",
    "We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the [autoregressive_sample_stream()](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream) decoding method from Trax to do fast inference. Let's define a few parameters to initialize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the `generate_dialogue()` function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat model-parts/chatbot_model1.pkl.gz.parta* > chatbot_model1.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few utility functions as well to help us tokenize and detokenize. We can use the [tokenize()](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize) and [detokenize()](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize) from `trax.data.tf_inputs` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, vocab_file, vocab_dir):\n",
    "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
    "\n",
    "def detokenize(tokens, vocab_file, vocab_dir):\n",
    "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a generator that predicts the next word of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
    "    \n",
    "    # Add batch dimension to array\n",
    "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
    "    \n",
    "    output_gen = trax.supervised.decoding.autoregressive_sample_stream(\n",
    "        model=ReformerLM,\n",
    "        inputs=input_tokens_with_batch,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return output_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function below will call the generator you just implemented and will just format the output to be easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "\n",
    "def attention(*args, **kwargs):\n",
    "    # number of input positions to remember in a cache when doing fast inference.\n",
    "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
    "    # number of input elements to drop once the fast inference input cache fills up.\n",
    "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
    "    return tl.SelfAttention(*args, **kwargs)\n",
    "\n",
    "model = ReformerLM(\n",
    "    vocab_size=33000,\n",
    "    n_layers=6,\n",
    "    mode='predict',\n",
    "    attention_type=attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holmen1/opt/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "# initialize from file\n",
    "model.init_from_file('chatbot_model1.pkl.gz',\n",
    "                     weights_only=True, input_signature=shape11)\n",
    "\n",
    "# save the starting state\n",
    "STARTING_STATE = model.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        model_state (np.array): initial state of the model before decoding\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        max_len (int): maximum number of tokens to generate \n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"  \n",
    "    \n",
    "    # define the delimiters we used during training\n",
    "    delimiter_1 = 'Person 1: ' \n",
    "    delimiter_2 = 'Person 2: '\n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # output tokens\n",
    "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
    "    \n",
    "    # reset the model state when starting a new dialogue\n",
    "    ReformerLM.state = model_state\n",
    "    \n",
    "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
    "    \n",
    "    print(start_sentence.split(delimiter_2)[0].strip())\n",
    "    \n",
    "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
    "    for o in output:\n",
    "        \n",
    "        result.append(o)\n",
    "        \n",
    "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
    "        \n",
    "        if sentence.endswith(delimiter_1):\n",
    "            sentence = sentence.split(delimiter_1)[0]\n",
    "            print(f'{delimiter_2}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "        \n",
    "        elif sentence.endswith(delimiter_2):\n",
    "            sentence = sentence.split(delimiter_2)[0]\n",
    "            print(f'{delimiter_1}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > max_len:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: Are there theatres in town?\n",
      "Person 2: : There are 5 theatres in town. Three are in the centre of town and 1 in the south. Do you have a preference? \n",
      "Person 1: No, but I would like the south please. \n",
      "Person 2: I have 4 theatres in the south. They are all in the south. Would you like more information on one of them? \n",
      "Person 1: Yes, I would like the postcode, and phone number please. \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: Is there a hospital nearby?\n",
      "Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need the main phone number? \n",
      "Person 1: No, I just need the general phone number, please. \n",
      "Person 2: The phone number is 01223245151. Do you need the main phone number? \n",
      "Person 1: No, that's all I needed. Thank you. \n",
      "Person 1: Okay, have a good day!!care!care of everything. Person 1 please. \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: Can you book a taxi?\n",
      "Person 2: : I sure can. When would you like to leave? \n",
      "Person 1: I need to leave after 13:00. \n",
      "Person 2: I've booked you a black volkswagen. The contact number is 0775077217261. \n",
      "Person 1: Thank you so much for your help. \n",
      "Person 2: You're welcome. Have a great day!Bye.\n",
      "Person 1: Thank you for your help. \n",
      "Person 1: You're welcome. Have a good night! \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-4"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
